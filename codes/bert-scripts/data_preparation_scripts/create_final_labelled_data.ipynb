{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84ff2836-5777-4401-b4ab-c6600bab0e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc46df2-de3d-442d-98df-15128c924245",
   "metadata": {},
   "source": [
    "# Generating parallel sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4009cba-fd61-45d3-bd1b-a221aede3955",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_path = \"word_aligner/All_sentence_files\"\n",
    "alignments_path = \"alignments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0510cd04-173f-4620-847b-8c86b9db9b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    f = open(path, 'r',encoding='utf8')\n",
    "    data = [l.strip() for l in f.readlines()]\n",
    "    f.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c8bf5c8-ba88-47af-95b3-c71ac6322bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = {}\n",
    "for file in glob.glob(sentences_path+\"/*\"):\n",
    "    lang = file.split(\"/\")[-1].split(\"_\")[0]\n",
    "    sentences[lang] = read_file(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4123e660-1c48-402e-aa6e-6c4adf854933",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_senteces = []\n",
    "for urdu_sent, hindi_sent, tamil_sent, telugu_sent in zip(sentences['urdu'], sentences['hindi'], sentences['tamil'], sentences['telugu']):\n",
    "    parallel_senteces.append({'urdu':urdu_sent,\n",
    "                             'hindi':hindi_sent,\n",
    "                             'tamil':tamil_sent,\n",
    "                             'telugu':telugu_sent})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3d43058-5f13-4479-a894-d7b02b1e581f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'urdu': 'چور سمجھ کر نامعلوم شخص کو دیہاتیوں نے بری طرح زد و کوب کرنے پر زیر علاج رہتے ہوئے فوت ہو گیا ۔',\n",
       " 'hindi': 'एक अज्ञात व्यक्ति को चोर समझकर ग्रामीणों ने जमकर पिटाई कर दी और इलाज के दौरान उसकी मौत हो गई.',\n",
       " 'tamil': 'அடையாளம் தெரியாத நபர், அவரை திருடன் என்று தவறாக நினைத்து, கிராம மக்களால் கடுமையாக தாக்கப்பட்டு, சிகிச்சை பலனின்றி உயிரிழந்தார்.',\n",
       " 'telugu': 'ఓ గుర్తుతెలియని వ్యక్తి దొంగగా భావించి గ్రామస్తులు తీవ్రంగా కొట్టి చికిత్స పొందుతూ మృతి చెందాడు.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_senteces[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51313561-a0f1-4454-ae1f-6483df3f9508",
   "metadata": {},
   "source": [
    "# Labelling translated sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e35d5c8-9501-4f52-b137-6304f5145ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1dd4c70-1248-4c2e-8041-24a3fc968b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "urdu_labels = []\n",
    "with open(\"json_data/ur_all.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        urdu_labels.append(json.loads(line))\n",
    "\n",
    "main_dict = {}\n",
    "for sample in urdu_labels:\n",
    "    sentence = \" \".join(sample['words'])\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    main_dict[sentence] = {'urdu':\n",
    "                           {\n",
    "                               'words':sample['words'], \n",
    "                               'srl':sample['srl']\n",
    "                           }\n",
    "                          }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0788130b-54d0-4447-a112-1e08a4e82ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_labels(source_labels, target_sentence, alignments, NAH_index = 16):\n",
    "    target_labels = [NAH_index]*len(target_sentence.split())\n",
    "    for i,j in alignments:\n",
    "        new_label = source_labels[i]\n",
    "        if new_label != NAH_index:\n",
    "            target_labels[j] = new_label\n",
    "    return target_labels\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "437ae3c4-573f-4fb1-94ce-ef3d0a446c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alignments/urdu-tamil.pkl tamil\n",
      "alignments/urdu-telugu.pkl telugu\n",
      "alignments/urdu-hindi.pkl hindi\n"
     ]
    }
   ],
   "source": [
    "problems = []\n",
    "success = []\n",
    "for file in glob.glob(alignments_path+\"/*\"):\n",
    "    lang = file.split(\"/\")[-1].split(\".\")[0].split(\"-\")[-1]\n",
    "    print(file, lang)\n",
    "    with open(file, \"rb\") as f:\n",
    "        alignments_pickle_load = pickle.load(f)\n",
    "\n",
    "    for sample in alignments_pickle_load:\n",
    "        try:\n",
    "            source_sent = sample['Source Sentence']\n",
    "            # source_sent = re.sub(r'\\s+', ' ', source_sent)\n",
    "            target_sent = sample['Target Sentence']\n",
    "            alignments = sample['Mapping/Alignment']\n",
    "            \n",
    "            source_labels = main_dict[source_sent]['urdu']['srl']\n",
    "            target_labels = generate_new_labels(source_labels, target_sent, alignments)\n",
    "    \n",
    "            main_dict[source_sent][lang] = {'words':target_sent.split(), 'srl':target_labels}\n",
    "            success.append(source_sent)\n",
    "        except Exception as e:\n",
    "            problems.append(source_sent)\n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "327a319c-ade0-41ec-adc9-a0af4787c020",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sents = list(main_dict.keys())\n",
    "import random\n",
    "random.shuffle(final_sents)\n",
    "random.shuffle(final_sents)\n",
    "random.shuffle(final_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6ee82d8-8113-4f77-98be-e14f0f13b444",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = len(final_sents)\n",
    "train_sents = final_sents[:int(l*0.7)]\n",
    "val_sents = final_sents[int(l*0.7):int(l*0.85)]\n",
    "test_sents = final_sents[int(l*0.85):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d19fdf42-7547-4ebc-9c9f-ea4a4cca2133",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fial_jsons/train_all.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sent in train_sents:\n",
    "        for lang in main_dict[sent]:\n",
    "            json.dump(main_dict[sent][lang], f, ensure_ascii=False)\n",
    "            f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0336c6e-1ab7-4d3e-a9f5-eff4b86438c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fial_jsons/val_all.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sent in val_sents:\n",
    "        for lang in main_dict[sent]:\n",
    "            json.dump(main_dict[sent][lang], f, ensure_ascii=False)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92d47b74-40e2-408b-ac45-27060bd40d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fial_jsons/test_urdu.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sent in test_sents:\n",
    "        json.dump(main_dict[sent]['urdu'], f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "with open(\"fial_jsons/test_hindi.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sent in test_sents:\n",
    "        json.dump(main_dict[sent]['hindi'], f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "\n",
    "with open(\"fial_jsons/test_tamil.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sent in test_sents:\n",
    "        json.dump(main_dict[sent]['tamil'], f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "with open(\"fial_jsons/test_telugu.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sent in test_sents:\n",
    "        json.dump(main_dict[sent]['telugu'], f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
